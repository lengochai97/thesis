{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "6.1. All Features.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lengochai97/thesis/blob/master/notebooks/feature_construction/61_All_Features.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x0S-b63NesNq",
        "colab_type": "text"
      },
      "source": [
        "# Connect to Google Drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mA8N4fsj8cY3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%capture\n",
        "\n",
        "import google.colab.drive\n",
        "\n",
        "google.colab.drive.mount('/content/gdrive', force_remount=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xR1ujuEeedLa",
        "colab_type": "text"
      },
      "source": [
        "# Install Spark and dependencies"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MzDLzKhZkCIf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "\n",
        "os.environ['HADOOP_VERSION'] = '2.7'\n",
        "os.environ['JAVA_HOME'] = '/usr/lib/jvm/java-8-openjdk-amd64'\n",
        "os.environ['SPARK_HOME'] = '/opt/spark'\n",
        "os.environ['SPARK_VERSION'] = '2.4.3'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F6xVWiMvy_cJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%capture\n",
        "\n",
        "!wget -qN https://archive.apache.org/dist/spark/spark-$SPARK_VERSION/spark-$SPARK_VERSION-bin-hadoop$HADOOP_VERSION.tgz\n",
        "!tar -xzf spark-$SPARK_VERSION-bin-hadoop$HADOOP_VERSION.tgz -C /opt\n",
        "!rm spark-$SPARK_VERSION-bin-hadoop$HADOOP_VERSION.tgz\n",
        "!rm -rf /opt/spark\n",
        "!ln -s /opt/spark-$SPARK_VERSION-bin-hadoop$HADOOP_VERSION /opt/spark\n",
        "!pip install -q findspark"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J13MUPQVtZiF",
        "colab_type": "text"
      },
      "source": [
        "# Create SparkSession"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q1Bm1_F32y9B",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import findspark\n",
        "\n",
        "findspark.init()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FvrAOXX10VgF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = SparkSession.builder.master('local[*]').config('spark.sql.autoBroadcastJoinThreshold', -1).getOrCreate()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2bLvSe5Nt5FX",
        "colab_type": "text"
      },
      "source": [
        "# Read files"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZUG5bFA3t-M2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import json\n",
        "\n",
        "import pyspark.sql.functions as F\n",
        "import pyspark.sql.types as T"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "141y1nsXt4lm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "DATA_PATH = '/content/gdrive/My Drive/dataset/adressa/one_week'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wfHKd9Ppwg4-",
        "colab_type": "text"
      },
      "source": [
        "## Cleaned"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ENiae8QQwoyL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with open(os.path.join(DATA_PATH, 'schema', 'clean.json')) as file:\n",
        "  clean_schema = T.StructType.fromJson(json.load(file))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZCV2keVxwx5L",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_clean = spark.read.json(os.path.join(DATA_PATH, 'clean'), schema=clean_schema)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TecjzrK312tN",
        "colab_type": "text"
      },
      "source": [
        "## Samples"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g-zaLwLx3yQP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with open(os.path.join(DATA_PATH, 'schema', 'samples.json')) as file:\n",
        "  sample_schema = T.StructType.fromJson(json.load(file))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O6-wFneM36Pv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_sample = spark.read.json(os.path.join(DATA_PATH, 'samples'), schema=sample_schema)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HnGG6wbB15nO",
        "colab_type": "text"
      },
      "source": [
        "## News features"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IGU1auv1GqA2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with open(os.path.join(DATA_PATH, 'schema', 'news_features.json')) as file:\n",
        "  news_features_schema = T.StructType.fromJson(json.load(file))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PLIHc7I4GtJT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_news_features = spark.read.json(os.path.join(DATA_PATH, 'news_features'), schema=news_features_schema)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VwQAF8Vh1_M-",
        "colab_type": "text"
      },
      "source": [
        "## User features"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iOVuTxnoLjR4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with open(os.path.join(DATA_PATH, 'schema', 'user_features.json')) as file:\n",
        "  user_features_schema = T.StructType.fromJson(json.load(file))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E6ZWBsvnLoad",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_user_features = spark.read.json(os.path.join(DATA_PATH, 'user_features'), schema=user_features_schema)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fBdT-aJe18Om",
        "colab_type": "text"
      },
      "source": [
        "## News click counts"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yrIXEwN1LUIe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with open(os.path.join(DATA_PATH, 'schema', 'news_click_counts.json')) as file:\n",
        "  news_click_counts_schema = T.StructType.fromJson(json.load(file))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a2tbFbMhLZaZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_news_click_counts = spark.read.json(os.path.join(DATA_PATH, 'news_click_counts'), schema=news_click_counts_schema)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XS4VcjWa2DO2",
        "colab_type": "text"
      },
      "source": [
        "## User activeness"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Ai0TF5R2F22",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with open(os.path.join(DATA_PATH, 'schema', 'user_activeness.json')) as file:\n",
        "  user_activeness_schema = T.StructType.fromJson(json.load(file))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gmJHQz1b2Jdp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_user_activeness = spark.read.json(os.path.join(DATA_PATH, 'user_activeness'), schema=user_activeness_schema)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M6kIUMYTbxe1",
        "colab_type": "text"
      },
      "source": [
        "# Construct user feature vectors"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NTX3vARfcYIU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from pyspark.ml.feature import CountVectorizerModel, Normalizer"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hEQr5BighMiY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "count_vectorizer_model_path = os.path.join(DATA_PATH, 'model', 'category_count_vectorizer')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X99bdRX3byX3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_sample_user_news = (\n",
        "    df_sample\n",
        "    .join(\n",
        "        df_user_features\n",
        "        .select(\n",
        "            F.column('userId'),\n",
        "            F.column('time'),\n",
        "            F.column('categoryListHistory'),\n",
        "            F.column('categoryList1H'),\n",
        "            F.column('categoryList6H'),\n",
        "            F.column('categoryList1D'),\n",
        "            F.column('categoryList1W'),\n",
        "            F.column('userClickCount1H'),\n",
        "            F.column('userClickCount6H'),\n",
        "            F.column('userClickCount1D'),\n",
        "            F.column('userClickCount1W'),\n",
        "        ),\n",
        "        on=['userId', 'time'],\n",
        "        how='inner',\n",
        "    )\n",
        "    .join(\n",
        "        df_news_features\n",
        "        .select(\n",
        "            F.column('newsId'),\n",
        "            F.column('categoryList'),\n",
        "            F.column('categoryVector'),\n",
        "        ),\n",
        "        on='newsId',\n",
        "        how='inner',\n",
        "    )\n",
        "    .withColumn(\n",
        "        'categoryListHistoryNext',\n",
        "        F.concat(F.column('categoryList'), F.column('categoryListHistory')),\n",
        "    )\n",
        "    .withColumn(\n",
        "        'categoryList1HNext',\n",
        "        F.concat(F.column('categoryList'), F.column('categoryList1H')),\n",
        "    )\n",
        "    .withColumn(\n",
        "        'categoryList6HNext',\n",
        "        F.concat(F.column('categoryList'), F.column('categoryList6H')),\n",
        "    )\n",
        "    .withColumn(\n",
        "        'categoryList1DNext',\n",
        "        F.concat(F.column('categoryList'), F.column('categoryList1D')),\n",
        "    )\n",
        "    .withColumn(\n",
        "        'categoryList1WNext',\n",
        "        F.concat(F.column('categoryList'), F.column('categoryList1W')),\n",
        "    )\n",
        "    .withColumn(\n",
        "        'userClickCount1HNext',\n",
        "        (F.column('userClickCount1H') + 1),\n",
        "    )\n",
        "    .withColumn(\n",
        "        'userClickCount6HNext',\n",
        "        (F.column('userClickCount6H') + 1),\n",
        "    )\n",
        "    .withColumn(\n",
        "        'userClickCount1DNext',\n",
        "        (F.column('userClickCount1D') + 1),\n",
        "    )\n",
        "    .withColumn(\n",
        "        'userClickCount1WNext',\n",
        "        (F.column('userClickCount1W') + 1),\n",
        "    )\n",
        ")\n",
        "\n",
        "# Build user feature vectors for current state\n",
        "\n",
        "df_sample_user_news = (\n",
        "    CountVectorizerModel\n",
        "    .load(count_vectorizer_model_path)\n",
        "    .setBinary(False)\n",
        "    .setInputCol('categoryListHistory')\n",
        "    .setOutputCol('userHistoryVector')\n",
        "    .transform(df_sample_user_news)\n",
        ")\n",
        "\n",
        "df_sample_user_news = (\n",
        "    CountVectorizerModel\n",
        "    .load(count_vectorizer_model_path)\n",
        "    .setBinary(True)\n",
        "    .setInputCol('categoryList1H')\n",
        "    .setOutputCol('userProfileVector1H')\n",
        "    .transform(df_sample_user_news)\n",
        ")\n",
        "\n",
        "df_sample_user_news = (\n",
        "    CountVectorizerModel\n",
        "    .load(count_vectorizer_model_path)\n",
        "    .setBinary(True)\n",
        "    .setInputCol('categoryList6H')\n",
        "    .setOutputCol('userProfileVector6H')\n",
        "    .transform(df_sample_user_news)\n",
        ")\n",
        "\n",
        "df_sample_user_news = (\n",
        "    CountVectorizerModel\n",
        "    .load(count_vectorizer_model_path)\n",
        "    .setBinary(True)\n",
        "    .setInputCol('categoryList1D')\n",
        "    .setOutputCol('userProfileVector1D')\n",
        "    .transform(df_sample_user_news)\n",
        ")\n",
        "\n",
        "df_sample_user_news = (\n",
        "    CountVectorizerModel\n",
        "    .load(count_vectorizer_model_path)\n",
        "    .setBinary(True)\n",
        "    .setInputCol('categoryList1W')\n",
        "    .setOutputCol('userProfileVector1W')\n",
        "    .transform(df_sample_user_news)\n",
        ")\n",
        "\n",
        "# Build user feature vectors for next state\n",
        "\n",
        "df_sample_user_news = (\n",
        "    CountVectorizerModel\n",
        "    .load(count_vectorizer_model_path)\n",
        "    .setBinary(False)\n",
        "    .setInputCol('categoryListHistoryNext')\n",
        "    .setOutputCol('userHistoryVectorNext')\n",
        "    .transform(df_sample_user_news)\n",
        ")\n",
        "\n",
        "df_sample_user_news = (\n",
        "    CountVectorizerModel\n",
        "    .load(count_vectorizer_model_path)\n",
        "    .setBinary(True)\n",
        "    .setInputCol('categoryList1HNext')\n",
        "    .setOutputCol('userProfileVector1HNext')\n",
        "    .transform(df_sample_user_news)\n",
        ")\n",
        "\n",
        "df_sample_user_news = (\n",
        "    CountVectorizerModel\n",
        "    .load(count_vectorizer_model_path)\n",
        "    .setBinary(True)\n",
        "    .setInputCol('categoryList6HNext')\n",
        "    .setOutputCol('userProfileVector6HNext')\n",
        "    .transform(df_sample_user_news)\n",
        ")\n",
        "\n",
        "df_sample_user_news = (\n",
        "    CountVectorizerModel\n",
        "    .load(count_vectorizer_model_path)\n",
        "    .setBinary(True)\n",
        "    .setInputCol('categoryList1DNext')\n",
        "    .setOutputCol('userProfileVector1DNext')\n",
        "    .transform(df_sample_user_news)\n",
        ")\n",
        "\n",
        "df_sample_user_news = (\n",
        "    CountVectorizerModel\n",
        "    .load(count_vectorizer_model_path)\n",
        "    .setBinary(True)\n",
        "    .setInputCol('categoryList1WNext')\n",
        "    .setOutputCol('userProfileVector1WNext')\n",
        "    .transform(df_sample_user_news)\n",
        ")\n",
        "\n",
        "# Normalize user history (optional)\n",
        "\n",
        "df_sample_user_news = (\n",
        "    Normalizer(\n",
        "        p=1.0,\n",
        "        inputCol='userHistoryVector',\n",
        "        outputCol='userHistoryVectorNormalized'\n",
        "    )\n",
        "    .transform(df_sample_user_news)\n",
        ")\n",
        "\n",
        "df_sample_user_news = (\n",
        "    Normalizer(\n",
        "        p=1.0,\n",
        "        inputCol='userHistoryVectorNext',\n",
        "        outputCol='userHistoryVectorNextNormalized'\n",
        "    )\n",
        "    .transform(df_sample_user_news)\n",
        ")\n",
        "\n",
        "# Filter columns\n",
        "\n",
        "df_sample_user_news = (\n",
        "    df_sample_user_news\n",
        "    .select(\n",
        "        F.column('userId'),\n",
        "        F.column('time'),\n",
        "        F.column('newsId'),\n",
        "        F.column('clickLabel'),\n",
        "        F.column('categoryVector'),\n",
        "        F.column('userHistoryVectorNormalized').alias('userHistoryVector'),\n",
        "        F.column('userProfileVector1H'),\n",
        "        F.column('userProfileVector6H'),\n",
        "        F.column('userProfileVector1D'),\n",
        "        F.column('userProfileVector1W'),\n",
        "        F.column('userClickCount1H'),\n",
        "        F.column('userClickCount6H'),\n",
        "        F.column('userClickCount1D'),\n",
        "        F.column('userClickCount1W'),\n",
        "        F.column('userHistoryVectorNextNormalized').alias('userHistoryVectorNext'),\n",
        "        F.column('userProfileVector1HNext'),\n",
        "        F.column('userProfileVector6HNext'),\n",
        "        F.column('userProfileVector1DNext'),\n",
        "        F.column('userProfileVector1WNext'),\n",
        "        F.column('userClickCount1HNext'),\n",
        "        F.column('userClickCount6HNext'),\n",
        "        F.column('userClickCount1DNext'),\n",
        "        F.column('userClickCount1WNext'),\n",
        "    )\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8eEDfrD5L1Zi",
        "colab_type": "text"
      },
      "source": [
        "# Construct context feature vector"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ENjfMrWXMLAC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from pyspark.ml.feature import OneHotEncoderEstimator"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "glaclUIMMPT7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_time_weekday = (\n",
        "    spark.range(0, 24)\n",
        "    .withColumnRenamed('id', 'timeContext')\n",
        "    .crossJoin(\n",
        "        spark.range(0, 7)\n",
        "        .withColumnRenamed('id', 'weekdayContext')\n",
        "    )\n",
        ")\n",
        "\n",
        "time_weekday_context_ohe = (\n",
        "    OneHotEncoderEstimator(\n",
        "        inputCols=['timeContext', 'weekdayContext'],\n",
        "        outputCols=['timeContextVector', 'weekdayContextVector'],\n",
        "        dropLast=False,\n",
        "    )\n",
        "    .fit(df_time_weekday)\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1RlJu4_jNH9p",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_sample_context = (\n",
        "    df_news_click_counts\n",
        "    .join(\n",
        "        df_news_features\n",
        "        .select(\n",
        "            F.column('newsId'),\n",
        "            F.column('publishtime'),\n",
        "        ),\n",
        "        on='newsId',\n",
        "        how='inner',\n",
        "    )\n",
        "    .withColumn(\n",
        "        'newsFreshnessContext',\n",
        "        (1. / (F.column('time') - F.column('publishtime'))),\n",
        "    )\n",
        "    .withColumn(\n",
        "        'timeContext',\n",
        "        F.hour(F.from_unixtime(F.column('time'))),\n",
        "    )\n",
        "    .withColumn(\n",
        "        'weekdayContext',\n",
        "        (F.dayofweek(F.from_unixtime(F.column('time'))) - 1),\n",
        "    )\n",
        ")\n",
        "\n",
        "df_sample_context = (\n",
        "    time_weekday_context_ohe\n",
        "    .transform(df_sample_context)\n",
        "    .select(\n",
        "        F.column('newsId'),\n",
        "        F.column('time'),\n",
        "        F.column('newsClickCount1H'),\n",
        "        F.column('newsClickCount6H'),\n",
        "        F.column('newsClickCount1D'),\n",
        "        F.column('newsClickCount1W'),\n",
        "        F.column('timeContextVector'),\n",
        "        F.column('weekdayContextVector'),\n",
        "        F.column('newsFreshnessContext'),\n",
        "    )\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9uBWj2Nh1Iyt",
        "colab_type": "text"
      },
      "source": [
        "# Construct `eventId`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3RfnAL7Vzzxu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from pyspark.sql import Window"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S3uGB-fTwbIU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_event_id = (\n",
        "    df_clean\n",
        "    .select(\n",
        "        F.column('userId'),\n",
        "        F.column('time'),\n",
        "    )\n",
        "    .withColumn(\n",
        "        'eventId',\n",
        "        F.row_number().over(\n",
        "            Window\n",
        "            .orderBy('time', 'userId')\n",
        "        ),\n",
        "    )\n",
        "    .withColumn(\n",
        "        'eventId',\n",
        "        (F.column('eventId') - 1),\n",
        "    )\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6KOH9kRL4BdU",
        "colab_type": "text"
      },
      "source": [
        "# Construct feature vectors for all features"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q_L-C4SvWNKg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from pyspark.ml.feature import VectorAssembler"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yU_JHDvCdK8h",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_all_features = (\n",
        "    df_sample_user_news\n",
        "    .join(\n",
        "        df_sample_context,\n",
        "        on=['newsId', 'time'],\n",
        "        how='inner',\n",
        "    )\n",
        "    .join(\n",
        "        df_user_activeness,\n",
        "        on=['userId', 'time'],\n",
        "        how='inner',\n",
        "    )\n",
        "    .join(\n",
        "        df_event_id,\n",
        "        on=['userId', 'time'],\n",
        "        how='inner',\n",
        "    )\n",
        ")\n",
        "\n",
        "# Build news feature vectors\n",
        "\n",
        "df_all_features = (\n",
        "    VectorAssembler(\n",
        "        inputCols=[\n",
        "            'newsClickCount1H',\n",
        "            'newsClickCount6H',\n",
        "            'newsClickCount1D',\n",
        "            'newsClickCount1W',\n",
        "        ],\n",
        "        outputCol='newsClickCountVector',\n",
        "    )\n",
        "    .transform(df_all_features)\n",
        ")\n",
        "\n",
        "# Build user feature vectors for current state\n",
        "\n",
        "df_all_features = (\n",
        "    VectorAssembler(\n",
        "        inputCols=[\n",
        "            'userProfileVector1H',\n",
        "            'userProfileVector6H',\n",
        "            'userProfileVector1D',\n",
        "            'userProfileVector1W',\n",
        "        ],\n",
        "        outputCol='userProfileVector',\n",
        "    )\n",
        "    .transform(df_all_features)\n",
        ")\n",
        "\n",
        "df_all_features = (\n",
        "    VectorAssembler(\n",
        "        inputCols=[\n",
        "            'userClickCount1H',\n",
        "            'userClickCount6H',\n",
        "            'userClickCount1D',\n",
        "            'userClickCount1W',\n",
        "        ],\n",
        "        outputCol='userClickCountVector',\n",
        "    )\n",
        "    .transform(df_all_features)\n",
        ")\n",
        "\n",
        "# Build user feature vectors for next state\n",
        "\n",
        "df_all_features = (\n",
        "    VectorAssembler(\n",
        "        inputCols=[\n",
        "            'userProfileVector1HNext',\n",
        "            'userProfileVector6HNext',\n",
        "            'userProfileVector1DNext',\n",
        "            'userProfileVector1WNext',\n",
        "        ],\n",
        "        outputCol='userProfileVectorNext',\n",
        "    )\n",
        "    .transform(df_all_features)\n",
        ")\n",
        "\n",
        "df_all_features = (\n",
        "    VectorAssembler(\n",
        "        inputCols=[\n",
        "            'userClickCount1HNext',\n",
        "            'userClickCount6HNext',\n",
        "            'userClickCount1DNext',\n",
        "            'userClickCount1WNext',\n",
        "        ],\n",
        "        outputCol='userClickCountVectorNext',\n",
        "    )\n",
        "    .transform(df_all_features)\n",
        ")\n",
        "\n",
        "# Build context feature vectors\n",
        "\n",
        "df_all_features = (\n",
        "    VectorAssembler(\n",
        "        inputCols=[\n",
        "            'timeContextVector',\n",
        "            'weekdayContextVector',\n",
        "            'newsFreshnessContext',\n",
        "        ],\n",
        "        outputCol='contextVector',\n",
        "    )\n",
        "    .transform(df_all_features)\n",
        ")\n",
        "\n",
        "# Filter columns\n",
        "\n",
        "df_all_features = (\n",
        "    df_all_features\n",
        "    .select(\n",
        "        F.column('eventId'),\n",
        "        F.column('userId'),\n",
        "        F.column('time'),\n",
        "        F.column('newsId'),\n",
        "        F.column('clickLabel'),\n",
        "        F.column('userActiveness'),\n",
        "        F.column('categoryVector'),\n",
        "        F.column('newsClickCountVector'),\n",
        "        F.column('contextVector'),\n",
        "        F.column('userHistoryVector'),\n",
        "        F.column('userProfileVector'),\n",
        "        F.column('userClickCountVector'),\n",
        "        F.column('userHistoryVectorNext'),\n",
        "        F.column('userProfileVectorNext'),\n",
        "        F.column('userClickCountVectorNext'),\n",
        "    )\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ssbv9mrM1cZ8",
        "colab_type": "text"
      },
      "source": [
        "# Train test split"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_7PhkW341eAM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_test_split_time = 1483743600 # 2017/01/06 23:00:00 UTC"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wc06urJX1olh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_all_features_train = df_all_features.filter(F.column('time') < train_test_split_time)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N-0R-12H1yvz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_all_features_test = df_all_features.filter(F.column('time') >= train_test_split_time)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_y4l_DDN4-9O",
        "colab_type": "text"
      },
      "source": [
        "# Write files"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PTIwxHxD14ei",
        "colab_type": "text"
      },
      "source": [
        "## Write train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m8NhhuOA17Yd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_all_features_train = (\n",
        "    df_all_features_train\n",
        "    .repartitionByRange(F.column('eventId'))\n",
        "    .sortWithinPartitions(F.column('eventId'))\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1SS_6pSSJNsO",
        "colab_type": "code",
        "outputId": "81fc2e95-4a61-4fae-b219-e22ba091c4a8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "%%time\n",
        "\n",
        "df_all_features_train.write.json(os.path.join(DATA_PATH, 'all_features', 'train'))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 343 ms, sys: 86.2 ms, total: 429 ms\n",
            "Wall time: 30min 50s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9WV8J-To2bD7",
        "colab_type": "text"
      },
      "source": [
        "## Write test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0kQkI_VC2caQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_all_features_test = (\n",
        "    df_all_features_test\n",
        "    .repartitionByRange(F.column('eventId'))\n",
        "    .sortWithinPartitions(F.column('eventId'))\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "smTl1vBT2fZ0",
        "colab_type": "code",
        "outputId": "5b49c1c1-b9b5-4e75-a742-532daa808502",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "%%time\n",
        "\n",
        "df_all_features_test.write.json(os.path.join(DATA_PATH, 'all_features', 'test'))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 212 ms, sys: 50.4 ms, total: 263 ms\n",
            "Wall time: 23min 35s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pqyctp7C2hxG",
        "colab_type": "text"
      },
      "source": [
        "## Write schema"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "upMd2mEqJRFc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with open(os.path.join(DATA_PATH, 'schema', 'all_features.json'), 'w+') as file:\n",
        "  json.dump(df_all_features.schema.jsonValue(), file)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}